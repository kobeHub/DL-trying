# Pooing Layer 池化层

## 简介：

池化（pooling）是卷及神经网络的一个重要概念，实际上是一种降采样(downsampling). 有多种形式的非线性池化函数．做窗口滑动卷积时，卷积值就代表了整个窗口的特征．因为滑动窗口间有大量重叠区域，所以计算的卷积值有冗余，进行pooling操作可以减少冗余，

减少冗余的同时，pooling 也丢掉了局部位置信息，所以局部有微小形变，得到的结果也是一样的，就像图片上的字母A发生微小变化也可以被识别为A．

而平移不变性，就是一个特征，无论出现在图片的任何位置，都会被识别出来，所以平移不变性不是池化带来的而是进行参数共享的依据

## 依据以及作用：

### 1.根据特征的invariance (不变性)

+ **translation invariance: (平移不变性)**
+ **rotation invariance(旋转不变性)**
+ **scale invariance(尺度不变性)**

可以进行池化操作，从而逐步减少表示空间的大小，以减少计算过程中的参数以及计算量．通过定期的在ConvNet体系结构中连续的卷积层之间插入池化层是一种常用的策略．池化在每个输入的深度切片中独立运行，控制过拟合

以maxpooling为例，最常见的是具有2x２的fliter 的池化层，分别沿height, width以步长为２进行选取最大值的操作．可以减少75%的数据激活．

- **Accepts a volume of size W1×H1×D1W1×H1×D1**

- **Requires two hyperparameters:**

  - 空间延展度，即filter的size　**F**,
  - 步长  **S**,

- **Produces a volume of size**  *W2×H2×D2W2×H2×D2*

  where:

  - *W2=(W1−F)/S+1W2=(W1−F)/S+1*
  - *H2=(H1−F)/S+1H2=(H1−F)/S+1*
  - *D2=D1*

- **Introduces zero parameters since it computes a fixed function of the input**

- **注意池化层一般不使用0界**

```
值得注意的是．只有两种常用的maxpooling参数选择
+ 重叠池化：f= 3, s =2
+ f= 2, s =2	标准池化，最常用
```

### 2.对于池化层的理解shiixanfenlei

进行平均池化(mean-pooling)时，往往能够保留数据的整体特征能够凸显其背景信息，而取最大值（max -pooling），则能更好的保留纹理上的信息．

## 3.对池化操作的废弃

很多人不喜欢使用池化层，比如在[strving for Simplicity :The All Convolution Net](https://arxiv.org/abs/1412.6806) 中提出了一个废弃池化层的方案：使用连续的卷积层构建神经网络，对于需要减少数据大小时，在一个卷积层中采用更大的步长．

在某些情形下，废弃pooing operation是很有必要的，比如训练好的生成模型：

Variational autoencoders(变分自编码器　VAEs)  

生成对抗网络：

Generative adversarial networks(GANs)

在我未来的体系结构中，可能很少或者不使用池化层

#全连接层　Fully-connected layer

位于全连接层中的神经元会连接来自上一层中的所有的激活元，如同RNN模型所示．在卷积神经网络中，全连接层主要有以下作用：

+ FC　在整个网络中起到了＂分类器＂的作用，在整体架构中，卷积层，池化层，激活函数层等操作可以将原始数据映射到隐藏空间，全连接层则起到[将学习到的＂分布式特征＂映射到样本标记空间]()

## Converting FC to Conv layers

值得注意的是，全连接层与卷积层的唯一不同是：卷积层中的神经元只与输入的一个局部区域相连接，而全连接层与上一层全部神经元相连接．但是在这两层中的神经元都是做点积，所以可以讲FC转化为卷积层

以VGG-16为例：

对于输入的224*224\*3  的图像，经过一系列的卷积层，池化层后获得的卷的大小为：7x7x512

(在AlexNet体系中，使用了5个池化层，每个池化层将输入进行空间上的降取样，每次为２　空间大小：

224/2/2/2/2/2 = 7）

然后再使用2层FC：每层大小4096

最后一层FC：1000个神经元，输出最后的类别分数

```python
## 可以进行下列的转化
#	１．将第一层的全连接层换为卷积核为 F = 7X7　的卷积层一共有4096个卷积核
#	2.根据第一层的输出[1x1x4096],第二层使用卷积核为 F = 1X1 共有　4096个卷积核的卷积层
#	3. 第二层的输出为［１ｘ１ｘ４０９６］ 再使用　１０００个［１ｘ１］的卷积核
#	４．得到最后[1x1x1000]的类别分数输出

```

