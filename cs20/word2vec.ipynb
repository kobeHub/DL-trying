{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Skip model\n",
    "\n",
    "\n",
    "## 1.  Word Embedding\n",
    "由于 onr-hot 映射的稀疏、词库巨大、不可以表示词的相关性等的缺点，可以使用词嵌入的方式对数据进行降维。一般使用词嵌入具有以下优势：\n",
    "\n",
    "   + 便于分布式 表示\n",
    "   + 可以使用连续值进行表示\n",
    "   + 更低的维度\n",
    "   + 可以获取词之间的语义关联\n",
    "    \n",
    "> ```shell\n",
    "Representing a word by means of its neighbors\n",
    "“You shall know a word by the company it keeps.”\n",
    "                                                                -- Firth, J. R. 1957:11\n",
    "```\n",
    "\n",
    "## 2. Word2Vec\n",
    "### 2.1 概述\n",
    "\n",
    "Word2Vec 使用了一个典型的机器学习中的降维的技巧，通过训练一个具有单隐含层的的神经网络。然后通过学习所得隐含层的权重作为实际的词向量，实现数据将为的目的。\n",
    "\n",
    "> 在非监督学习中，Auto-encoder 也使用了这样的技巧。可以使用自动编码机将输入向量进行压缩；使其在隐含层降维；在输出层可以将其解压缩到原来的向量，训练结束之后可以剥离输出层，使用隐含层。可以通过这种方式学习良好的图像特征而无需标记训练数据的技巧。\n",
    "\n",
    "word2vec 具有两个模型，CBOW and skip-gram. 两个模型原理相似，唯一不同之处在于，连续词包模型是根据上下文预测一个词，而跳表模型是根据一个中心词预测其上下文。从数据分析上说， CBOW 模型具有平滑大量分布信息的效果（将上下文视为一个观察），通常对于小数据集是有效的。但是 skip-gram 将每一个 `context-target` 视为一个观察，通常在大数据集上的表现更好。\n",
    "\n",
    "### 2.2 skip-gram 基本概念\n",
    "需要构建一个神经网络去完成这样一个任务：给定一个句子的中心词，随机查找并挑选其附近的词。神经网络需要告诉我们词库中的每一个词出现在中心词附近的概率。\n",
    "\n",
    "附近的词需要使用**窗口（window）**的概念描述：\n",
    " > window size: 需要考虑中心词左边w，以及右边w个词；一般不大于5\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![skip-gram](http://mccormickml.com/assets/word2vec/training_data.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 skip-gram 细节\n",
    "首先需要创建一个词库，使用 one-hot 编码，对于一个大小为N的词库而言，每一个词都是N维向量。作为模型的输入。模型的输出也是一个 N维 向量，包含了每一个词出现在中心词 **附近** 的概率。基本的结构如下：\n",
    "![arch](http://mccormickml.com/assets/word2vec/skip_gram_net_arch.png)\n",
    "\n",
    "以上模型没有使用非线性激活函数， `Output Layer` 使用了softmax，用以表示所有词与中心词的相关性。训练以上模型时，输入是一个 one-hot vector, 输出是一个概率分布函数。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在以上模型中，如果需要训练的词向量有300个特征，或者说将原有的 N-d vector 映射到一个 300 维的向量空间中。那么隐含层的 `shape =[N, 300]` . **隐含层就是我们需要的词向量查找表**\n",
    "\n",
    "![look](http://mccormickml.com/assets/word2vec/word2vec_weight_matrix_lookup_table.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Output Layer\n",
    "输出层使用一个 `softmax regression classfier`, 也就是多元逻辑回归分类器。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. tensorflow 操作\n",
    "\n",
    "### 3.1 Embedding Lookup Ops\n",
    "\n",
    "$[0\\;0\\;0\\; 1\\; 0]\\quad \\times \\quad  \\left[ \\begin{matrix}\n",
    "   17 & 24 & 1 \\\\\n",
    "   23 & 5 & 7 \\\\\n",
    "   4 & 6 & 13 \\\\\n",
    "   \\color{green}{10} & \\color{green}{12} & \\color{green}{19} \\\\\n",
    "   11 & 18 & 25\n",
    "  \\end{matrix} \\right]  = [10 \\;12 \\; 19]\\tag{1}$\n",
    " \n",
    " > tf.nn.embedding_lookup(params, ids, partition_strategy='mod', name=None, validate_indices=True, max_norm=None)\n",
    " \n",
    "\n",
    " \n",
    " ### 3.2 NCE Loss function\n",
    " 关于损失函数的选取，可以使用 softmax ，但是需要大量的计算，可以使用基于取样的方式。\n",
    " **负取样（Negative Sampling）** 是一种简化的 **噪声对比预测 NCE（Noise Contrastive Estimation）**。 使用NCE可以近似 softmax，但是使用负取样不可以。[CS224N](http://web.stanford.edu/class/cs224n/) \n",
    "\n",
    "> nce_loss(weights, biases, labels, inputs, num_sampled, num_classes, num_true=1, sampled_values=None, remove_accidental_hits=False, partition_strategy='mod', name='nce_loss')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "## 4. 构建模型\n",
    "为了使模型可复用，可以试用一下策略：\n",
    "+ 为模型定义一个类\n",
    "+ 在集合类型中建立模型\n",
    "+ 将模型的 `graph_def` 存储在文件中，需要使用时不需要重建\n",
    "\n",
    "\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Word2Vec skip-gram with NCE loss function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.contrib.tensorboard.plugins import projector\n",
    "\n",
    "import utils\n",
    "import word2vec_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "设置模型超参数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = 50_000\n",
    "BATCH_SIZE = 128\n",
    "EMBED_SIZE = 128     # demision of the word embedding\n",
    "SKIP_WINDOW = 1      # 上下文窗口大小\n",
    "NUM_SAMPLED = 64  # 负取样的大小\n",
    "lr = 1.0\n",
    "NUM_TRAIN_STEP = 100_000\n",
    "VISUAL_FLD = 'visualization'\n",
    "SKIP_STEP = 5_000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "需要下载的数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNLOAD_URL = 'http://mattmahoney.net/dc/text8.zip'\n",
    "EXPECTED_BYTES = 31344016\n",
    "NUM_VISUALIZE = 3000        # number of tokens to visualize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用函数式定义模型："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def word2vec(dataset):\n",
    "    \"\"\"\n",
    "    dataset: The input, output tf.data.Dataset\n",
    "    \"\"\"\n",
    "    # get input ,output  from dataset\n",
    "    with tf.name_scope('data'):\n",
    "        iterator = dataset.make_initializable_iterator()\n",
    "        center_words, target_words = iterator.get_next()\n",
    "        \n",
    "    # Define weights and embedding look up\n",
    "    with tf.name_scope('embedd'):\n",
    "        embed_matrix = tf.get_variable('embed_matrix',\n",
    "                                      shape=[VOCAB_SIZE, EMBED_SIZE],\n",
    "                                      initializer=tf.random_uniform_initializer())\n",
    "        embed = tf.nn.embedding_lookup(embed_matrix, center_words, name=\"embedding\")\n",
    "    \n",
    "    # Construct variables for NCE loss and define loss function\n",
    "    with tf.name_scope('loss'):\n",
    "        nce_weight = tf.get_variable('nce_weight',\n",
    "                                    shape=[VOCAB_SIZE, EMBED_SIZE],\n",
    "                                    initializer=tf.truncated_normal_initializer(stddev=1.0 / (EMBED_SIZE ** 0.5)))\n",
    "        nce_bias = tf.get_variable('nce_bias', \n",
    "                                  initializer=tf.zeros([VOCAB_SIZE]))\n",
    "        \n",
    "        #define loss \n",
    "        loss = tf.reduce_mean(tf.nn.nce_loss(\n",
    "            weights=nce_weight,\n",
    "            biases=nce_bias,\n",
    "            inputs=embed,\n",
    "            labels=target_words,\n",
    "            num_sampled=NUM_SAMPLED,\n",
    "            num_classes=VOCAB_SIZE), name='loss')\n",
    "        \n",
    "    with tf.name_scope('optimizer'):\n",
    "        optimizer =tf.train.GradientDescentOptimizer(lr).minimize(loss)\n",
    "        \n",
    "    utils.safe_mkdir('checkpoints')\n",
    "    \n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        sess.run(iterator.initializer)\n",
    "        \n",
    "        total_loss = 0.0 \n",
    "        writer = tf.summary.FileWriter('graphs/word2vec_simple', sess.graph)\n",
    "        \n",
    "        for i in range(NUM_TRAIN_STEP):\n",
    "            try:\n",
    "                loss_batch, _ = sess.run([loss, optimizer])\n",
    "                total_loss += loss_batch\n",
    "                if (i + 1) % SKIP_STEP == 0:\n",
    "                    print(f'Average loss at step {i}: {total_loss / SKIP_STEP:5.1f}')\n",
    "                    total_loss = 0\n",
    "            except tf.errors.OutOfRangeError:\n",
    "                sess.run(iterator.initializer)\n",
    "        writer.close()\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "从生成器中获取数据："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gen():\n",
    "    yield from word2vec_utils.batch_gen(DOWNLOAD_URL, EXPECTED_BYTES, VOCAB_SIZE, \n",
    "                                        BATCH_SIZE, SKIP_WINDOW, VISUAL_FLD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    dataset = tf.data.Dataset.from_generator(gen, \n",
    "                                (tf.int32, tf.int32), \n",
    "                                (tf.TensorShape([BATCH_SIZE]), tf.TensorShape([BATCH_SIZE, 1])))\n",
    "    word2vec(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /usr/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py:429: py_func (from tensorflow.python.ops.script_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "tf.py_func is deprecated in TF V2. Instead, use\n",
      "    tf.py_function, which takes a python function which manipulates tf eager\n",
      "    tensors instead of numpy arrays. It's easy to convert a tf eager tensor to\n",
      "    an ndarray (just call tensor.numpy()) but having access to eager tensors\n",
      "    means `tf.py_function`s can use accelerators such as GPUs as well as\n",
      "    being differentiable using a gradient tape.\n",
      "    \n",
      "WARNING:tensorflow:From /usr/lib/python3.7/site-packages/tensorflow/python/data/ops/dataset_ops.py:1419: colocate_with (from tensorflow.python.framework.ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Colocations handled automatically by placer.\n",
      "WARNING:tensorflow:From /usr/lib/python3.7/site-packages/tensorflow/python/ops/array_grad.py:425: to_int32 (from tensorflow.python.ops.math_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.cast instead.\n",
      "Downloading http://mattmahoney.net/dc/text8.zip\n",
      "Successfully downloaded data/text8.zip\n",
      "Average loss at step 4999:  65.1\n",
      "Average loss at step 9999:  18.5\n",
      "Average loss at step 14999:   9.6\n",
      "Average loss at step 19999:   6.7\n",
      "Average loss at step 24999:   5.7\n",
      "Average loss at step 29999:   5.2\n",
      "Average loss at step 34999:   5.0\n",
      "Average loss at step 39999:   4.8\n",
      "Average loss at step 44999:   4.8\n",
      "Average loss at step 49999:   4.8\n",
      "Average loss at step 54999:   4.7\n",
      "Average loss at step 59999:   4.7\n",
      "Average loss at step 64999:   4.6\n",
      "Average loss at step 69999:   4.6\n",
      "Average loss at step 74999:   4.6\n",
      "Average loss at step 79999:   4.6\n",
      "Average loss at step 84999:   4.7\n",
      "Average loss at step 89999:   4.7\n",
      "Average loss at step 94999:   4.6\n",
      "Average loss at step 99999:   4.6\n"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
